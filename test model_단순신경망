{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvDLVki6x37VPiFo/97+u9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["clear"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WQiwfR-wQlJ0","executionInfo":{"status":"ok","timestamp":1668666531801,"user_tz":-540,"elapsed":330,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"08311580-846d-4f1f-a986-b2b949acf46c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[H\u001b[2J"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9zqe1a4pqCZ","executionInfo":{"status":"ok","timestamp":1668666858828,"user_tz":-540,"elapsed":323555,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"6f96f2d1-3c90-4ae6-e0aa-fbb2897a5ff2"},"outputs":[{"output_type":"stream","name":"stdout","text":["60000 train samples\n","10000 test samples\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_layer (Dense)         (None, 10)                7850      \n","                                                                 \n","=================================================================\n","Total params: 7,850\n","Trainable params: 7,850\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/200\n","375/375 [==============================] - 2s 4ms/step - loss: 1.4313 - accuracy: 0.6349 - val_loss: 0.9142 - val_accuracy: 0.8176\n","Epoch 2/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.8065 - accuracy: 0.8225 - val_loss: 0.6668 - val_accuracy: 0.8525\n","Epoch 3/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.8467 - val_loss: 0.5682 - val_accuracy: 0.8672\n","Epoch 4/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.5769 - accuracy: 0.8586 - val_loss: 0.5137 - val_accuracy: 0.8748\n","Epoch 5/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.5314 - accuracy: 0.8666 - val_loss: 0.4786 - val_accuracy: 0.8799\n","Epoch 6/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.5004 - accuracy: 0.8726 - val_loss: 0.4539 - val_accuracy: 0.8839\n","Epoch 7/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4774 - accuracy: 0.8766 - val_loss: 0.4353 - val_accuracy: 0.8875\n","Epoch 8/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4596 - accuracy: 0.8795 - val_loss: 0.4206 - val_accuracy: 0.8896\n","Epoch 9/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.4453 - accuracy: 0.8823 - val_loss: 0.4088 - val_accuracy: 0.8921\n","Epoch 10/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.4334 - accuracy: 0.8845 - val_loss: 0.3989 - val_accuracy: 0.8954\n","Epoch 11/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.4234 - accuracy: 0.8866 - val_loss: 0.3907 - val_accuracy: 0.8966\n","Epoch 12/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.4148 - accuracy: 0.8888 - val_loss: 0.3834 - val_accuracy: 0.8992\n","Epoch 13/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.4073 - accuracy: 0.8899 - val_loss: 0.3774 - val_accuracy: 0.8999\n","Epoch 14/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.4008 - accuracy: 0.8917 - val_loss: 0.3717 - val_accuracy: 0.9008\n","Epoch 15/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3948 - accuracy: 0.8928 - val_loss: 0.3668 - val_accuracy: 0.9021\n","Epoch 16/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3895 - accuracy: 0.8940 - val_loss: 0.3625 - val_accuracy: 0.9031\n","Epoch 17/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3847 - accuracy: 0.8950 - val_loss: 0.3585 - val_accuracy: 0.9028\n","Epoch 18/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3804 - accuracy: 0.8959 - val_loss: 0.3551 - val_accuracy: 0.9034\n","Epoch 19/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3764 - accuracy: 0.8965 - val_loss: 0.3516 - val_accuracy: 0.9050\n","Epoch 20/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3728 - accuracy: 0.8977 - val_loss: 0.3485 - val_accuracy: 0.9052\n","Epoch 21/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3694 - accuracy: 0.8981 - val_loss: 0.3458 - val_accuracy: 0.9062\n","Epoch 22/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3662 - accuracy: 0.8992 - val_loss: 0.3433 - val_accuracy: 0.9064\n","Epoch 23/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3633 - accuracy: 0.8995 - val_loss: 0.3408 - val_accuracy: 0.9073\n","Epoch 24/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3605 - accuracy: 0.9001 - val_loss: 0.3386 - val_accuracy: 0.9073\n","Epoch 25/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3580 - accuracy: 0.9009 - val_loss: 0.3364 - val_accuracy: 0.9085\n","Epoch 26/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3556 - accuracy: 0.9016 - val_loss: 0.3345 - val_accuracy: 0.9087\n","Epoch 27/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3533 - accuracy: 0.9022 - val_loss: 0.3327 - val_accuracy: 0.9092\n","Epoch 28/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3511 - accuracy: 0.9028 - val_loss: 0.3309 - val_accuracy: 0.9097\n","Epoch 29/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3491 - accuracy: 0.9030 - val_loss: 0.3293 - val_accuracy: 0.9095\n","Epoch 30/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3471 - accuracy: 0.9039 - val_loss: 0.3278 - val_accuracy: 0.9100\n","Epoch 31/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3454 - accuracy: 0.9036 - val_loss: 0.3261 - val_accuracy: 0.9100\n","Epoch 32/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3436 - accuracy: 0.9045 - val_loss: 0.3248 - val_accuracy: 0.9095\n","Epoch 33/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3419 - accuracy: 0.9053 - val_loss: 0.3234 - val_accuracy: 0.9101\n","Epoch 34/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3403 - accuracy: 0.9054 - val_loss: 0.3222 - val_accuracy: 0.9106\n","Epoch 35/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3388 - accuracy: 0.9056 - val_loss: 0.3209 - val_accuracy: 0.9118\n","Epoch 36/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.3373 - accuracy: 0.9063 - val_loss: 0.3197 - val_accuracy: 0.9118\n","Epoch 37/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3359 - accuracy: 0.9065 - val_loss: 0.3186 - val_accuracy: 0.9124\n","Epoch 38/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3346 - accuracy: 0.9072 - val_loss: 0.3176 - val_accuracy: 0.9122\n","Epoch 39/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3333 - accuracy: 0.9075 - val_loss: 0.3166 - val_accuracy: 0.9121\n","Epoch 40/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3320 - accuracy: 0.9078 - val_loss: 0.3156 - val_accuracy: 0.9124\n","Epoch 41/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3309 - accuracy: 0.9080 - val_loss: 0.3145 - val_accuracy: 0.9127\n","Epoch 42/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3297 - accuracy: 0.9082 - val_loss: 0.3136 - val_accuracy: 0.9126\n","Epoch 43/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3286 - accuracy: 0.9087 - val_loss: 0.3128 - val_accuracy: 0.9129\n","Epoch 44/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3275 - accuracy: 0.9088 - val_loss: 0.3119 - val_accuracy: 0.9132\n","Epoch 45/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3264 - accuracy: 0.9091 - val_loss: 0.3111 - val_accuracy: 0.9135\n","Epoch 46/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3254 - accuracy: 0.9096 - val_loss: 0.3104 - val_accuracy: 0.9135\n","Epoch 47/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3244 - accuracy: 0.9095 - val_loss: 0.3096 - val_accuracy: 0.9138\n","Epoch 48/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3235 - accuracy: 0.9098 - val_loss: 0.3089 - val_accuracy: 0.9140\n","Epoch 49/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3226 - accuracy: 0.9100 - val_loss: 0.3081 - val_accuracy: 0.9143\n","Epoch 50/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3217 - accuracy: 0.9105 - val_loss: 0.3075 - val_accuracy: 0.9139\n","Epoch 51/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3208 - accuracy: 0.9106 - val_loss: 0.3067 - val_accuracy: 0.9144\n","Epoch 52/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3200 - accuracy: 0.9110 - val_loss: 0.3061 - val_accuracy: 0.9147\n","Epoch 53/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3192 - accuracy: 0.9110 - val_loss: 0.3054 - val_accuracy: 0.9148\n","Epoch 54/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.3184 - accuracy: 0.9116 - val_loss: 0.3049 - val_accuracy: 0.9148\n","Epoch 55/200\n","375/375 [==============================] - 3s 7ms/step - loss: 0.3176 - accuracy: 0.9116 - val_loss: 0.3042 - val_accuracy: 0.9157\n","Epoch 56/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3169 - accuracy: 0.9118 - val_loss: 0.3036 - val_accuracy: 0.9154\n","Epoch 57/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3161 - accuracy: 0.9119 - val_loss: 0.3030 - val_accuracy: 0.9155\n","Epoch 58/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3154 - accuracy: 0.9123 - val_loss: 0.3025 - val_accuracy: 0.9158\n","Epoch 59/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3147 - accuracy: 0.9125 - val_loss: 0.3020 - val_accuracy: 0.9158\n","Epoch 60/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3140 - accuracy: 0.9124 - val_loss: 0.3015 - val_accuracy: 0.9166\n","Epoch 61/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3134 - accuracy: 0.9128 - val_loss: 0.3009 - val_accuracy: 0.9162\n","Epoch 62/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3127 - accuracy: 0.9130 - val_loss: 0.3005 - val_accuracy: 0.9164\n","Epoch 63/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3120 - accuracy: 0.9134 - val_loss: 0.3001 - val_accuracy: 0.9167\n","Epoch 64/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.3115 - accuracy: 0.9134 - val_loss: 0.2995 - val_accuracy: 0.9168\n","Epoch 65/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3109 - accuracy: 0.9136 - val_loss: 0.2991 - val_accuracy: 0.9170\n","Epoch 66/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3103 - accuracy: 0.9138 - val_loss: 0.2986 - val_accuracy: 0.9172\n","Epoch 67/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3097 - accuracy: 0.9139 - val_loss: 0.2982 - val_accuracy: 0.9172\n","Epoch 68/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3091 - accuracy: 0.9138 - val_loss: 0.2978 - val_accuracy: 0.9170\n","Epoch 69/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3086 - accuracy: 0.9141 - val_loss: 0.2973 - val_accuracy: 0.9167\n","Epoch 70/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.3080 - accuracy: 0.9143 - val_loss: 0.2970 - val_accuracy: 0.9173\n","Epoch 71/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3075 - accuracy: 0.9143 - val_loss: 0.2966 - val_accuracy: 0.9177\n","Epoch 72/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3070 - accuracy: 0.9146 - val_loss: 0.2961 - val_accuracy: 0.9176\n","Epoch 73/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3065 - accuracy: 0.9150 - val_loss: 0.2958 - val_accuracy: 0.9176\n","Epoch 74/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3060 - accuracy: 0.9148 - val_loss: 0.2954 - val_accuracy: 0.9175\n","Epoch 75/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3055 - accuracy: 0.9151 - val_loss: 0.2950 - val_accuracy: 0.9177\n","Epoch 76/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.3050 - accuracy: 0.9152 - val_loss: 0.2946 - val_accuracy: 0.9175\n","Epoch 77/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.3045 - accuracy: 0.9152 - val_loss: 0.2943 - val_accuracy: 0.9180\n","Epoch 78/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.3040 - accuracy: 0.9153 - val_loss: 0.2941 - val_accuracy: 0.9182\n","Epoch 79/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3036 - accuracy: 0.9153 - val_loss: 0.2936 - val_accuracy: 0.9184\n","Epoch 80/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3032 - accuracy: 0.9157 - val_loss: 0.2934 - val_accuracy: 0.9181\n","Epoch 81/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3028 - accuracy: 0.9156 - val_loss: 0.2930 - val_accuracy: 0.9179\n","Epoch 82/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.3023 - accuracy: 0.9157 - val_loss: 0.2927 - val_accuracy: 0.9182\n","Epoch 83/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3019 - accuracy: 0.9160 - val_loss: 0.2925 - val_accuracy: 0.9182\n","Epoch 84/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3015 - accuracy: 0.9160 - val_loss: 0.2922 - val_accuracy: 0.9188\n","Epoch 85/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3011 - accuracy: 0.9160 - val_loss: 0.2919 - val_accuracy: 0.9185\n","Epoch 86/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3007 - accuracy: 0.9161 - val_loss: 0.2915 - val_accuracy: 0.9187\n","Epoch 87/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.3003 - accuracy: 0.9162 - val_loss: 0.2912 - val_accuracy: 0.9191\n","Epoch 88/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2999 - accuracy: 0.9164 - val_loss: 0.2909 - val_accuracy: 0.9191\n","Epoch 89/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2995 - accuracy: 0.9163 - val_loss: 0.2906 - val_accuracy: 0.9190\n","Epoch 90/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2992 - accuracy: 0.9165 - val_loss: 0.2904 - val_accuracy: 0.9187\n","Epoch 91/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2988 - accuracy: 0.9164 - val_loss: 0.2901 - val_accuracy: 0.9195\n","Epoch 92/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2984 - accuracy: 0.9168 - val_loss: 0.2899 - val_accuracy: 0.9193\n","Epoch 93/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.2981 - accuracy: 0.9167 - val_loss: 0.2896 - val_accuracy: 0.9192\n","Epoch 94/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.2977 - accuracy: 0.9166 - val_loss: 0.2893 - val_accuracy: 0.9196\n","Epoch 95/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.2973 - accuracy: 0.9170 - val_loss: 0.2891 - val_accuracy: 0.9197\n","Epoch 96/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2970 - accuracy: 0.9170 - val_loss: 0.2889 - val_accuracy: 0.9198\n","Epoch 97/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2967 - accuracy: 0.9173 - val_loss: 0.2886 - val_accuracy: 0.9197\n","Epoch 98/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2963 - accuracy: 0.9173 - val_loss: 0.2885 - val_accuracy: 0.9193\n","Epoch 99/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2960 - accuracy: 0.9172 - val_loss: 0.2881 - val_accuracy: 0.9197\n","Epoch 100/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.9176 - val_loss: 0.2879 - val_accuracy: 0.9197\n","Epoch 101/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2954 - accuracy: 0.9175 - val_loss: 0.2877 - val_accuracy: 0.9197\n","Epoch 102/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2951 - accuracy: 0.9175 - val_loss: 0.2874 - val_accuracy: 0.9196\n","Epoch 103/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2948 - accuracy: 0.9179 - val_loss: 0.2872 - val_accuracy: 0.9199\n","Epoch 104/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2945 - accuracy: 0.9177 - val_loss: 0.2870 - val_accuracy: 0.9202\n","Epoch 105/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2941 - accuracy: 0.9176 - val_loss: 0.2869 - val_accuracy: 0.9201\n","Epoch 106/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2939 - accuracy: 0.9180 - val_loss: 0.2866 - val_accuracy: 0.9202\n","Epoch 107/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2936 - accuracy: 0.9184 - val_loss: 0.2865 - val_accuracy: 0.9203\n","Epoch 108/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2932 - accuracy: 0.9183 - val_loss: 0.2864 - val_accuracy: 0.9197\n","Epoch 109/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2930 - accuracy: 0.9185 - val_loss: 0.2861 - val_accuracy: 0.9206\n","Epoch 110/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2927 - accuracy: 0.9183 - val_loss: 0.2859 - val_accuracy: 0.9202\n","Epoch 111/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2925 - accuracy: 0.9186 - val_loss: 0.2857 - val_accuracy: 0.9205\n","Epoch 112/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2922 - accuracy: 0.9185 - val_loss: 0.2855 - val_accuracy: 0.9209\n","Epoch 113/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2919 - accuracy: 0.9185 - val_loss: 0.2854 - val_accuracy: 0.9202\n","Epoch 114/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2916 - accuracy: 0.9187 - val_loss: 0.2852 - val_accuracy: 0.9208\n","Epoch 115/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2913 - accuracy: 0.9188 - val_loss: 0.2849 - val_accuracy: 0.9201\n","Epoch 116/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2911 - accuracy: 0.9186 - val_loss: 0.2848 - val_accuracy: 0.9209\n","Epoch 117/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2909 - accuracy: 0.9190 - val_loss: 0.2846 - val_accuracy: 0.9208\n","Epoch 118/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2906 - accuracy: 0.9190 - val_loss: 0.2844 - val_accuracy: 0.9208\n","Epoch 119/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2903 - accuracy: 0.9190 - val_loss: 0.2842 - val_accuracy: 0.9208\n","Epoch 120/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2901 - accuracy: 0.9190 - val_loss: 0.2841 - val_accuracy: 0.9210\n","Epoch 121/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2898 - accuracy: 0.9191 - val_loss: 0.2839 - val_accuracy: 0.9210\n","Epoch 122/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2896 - accuracy: 0.9194 - val_loss: 0.2837 - val_accuracy: 0.9211\n","Epoch 123/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2894 - accuracy: 0.9194 - val_loss: 0.2836 - val_accuracy: 0.9208\n","Epoch 124/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2891 - accuracy: 0.9194 - val_loss: 0.2834 - val_accuracy: 0.9210\n","Epoch 125/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2889 - accuracy: 0.9196 - val_loss: 0.2833 - val_accuracy: 0.9212\n","Epoch 126/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2887 - accuracy: 0.9197 - val_loss: 0.2831 - val_accuracy: 0.9211\n","Epoch 127/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2884 - accuracy: 0.9197 - val_loss: 0.2830 - val_accuracy: 0.9213\n","Epoch 128/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2882 - accuracy: 0.9197 - val_loss: 0.2829 - val_accuracy: 0.9211\n","Epoch 129/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2880 - accuracy: 0.9196 - val_loss: 0.2827 - val_accuracy: 0.9212\n","Epoch 130/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2877 - accuracy: 0.9200 - val_loss: 0.2826 - val_accuracy: 0.9211\n","Epoch 131/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2876 - accuracy: 0.9196 - val_loss: 0.2824 - val_accuracy: 0.9209\n","Epoch 132/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2873 - accuracy: 0.9198 - val_loss: 0.2823 - val_accuracy: 0.9212\n","Epoch 133/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2871 - accuracy: 0.9199 - val_loss: 0.2821 - val_accuracy: 0.9212\n","Epoch 134/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2869 - accuracy: 0.9199 - val_loss: 0.2820 - val_accuracy: 0.9214\n","Epoch 135/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2867 - accuracy: 0.9199 - val_loss: 0.2818 - val_accuracy: 0.9211\n","Epoch 136/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2865 - accuracy: 0.9199 - val_loss: 0.2816 - val_accuracy: 0.9216\n","Epoch 137/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.2863 - accuracy: 0.9202 - val_loss: 0.2815 - val_accuracy: 0.9216\n","Epoch 138/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.2861 - accuracy: 0.9201 - val_loss: 0.2814 - val_accuracy: 0.9212\n","Epoch 139/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2859 - accuracy: 0.9203 - val_loss: 0.2813 - val_accuracy: 0.9212\n","Epoch 140/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2857 - accuracy: 0.9202 - val_loss: 0.2811 - val_accuracy: 0.9214\n","Epoch 141/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2855 - accuracy: 0.9201 - val_loss: 0.2810 - val_accuracy: 0.9212\n","Epoch 142/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2853 - accuracy: 0.9204 - val_loss: 0.2809 - val_accuracy: 0.9215\n","Epoch 143/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2851 - accuracy: 0.9202 - val_loss: 0.2808 - val_accuracy: 0.9216\n","Epoch 144/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2849 - accuracy: 0.9205 - val_loss: 0.2806 - val_accuracy: 0.9217\n","Epoch 145/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2847 - accuracy: 0.9204 - val_loss: 0.2805 - val_accuracy: 0.9216\n","Epoch 146/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2845 - accuracy: 0.9207 - val_loss: 0.2804 - val_accuracy: 0.9215\n","Epoch 147/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2843 - accuracy: 0.9202 - val_loss: 0.2802 - val_accuracy: 0.9214\n","Epoch 148/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2841 - accuracy: 0.9204 - val_loss: 0.2801 - val_accuracy: 0.9217\n","Epoch 149/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2840 - accuracy: 0.9203 - val_loss: 0.2800 - val_accuracy: 0.9218\n","Epoch 150/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2838 - accuracy: 0.9208 - val_loss: 0.2799 - val_accuracy: 0.9217\n","Epoch 151/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2836 - accuracy: 0.9206 - val_loss: 0.2798 - val_accuracy: 0.9214\n","Epoch 152/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2834 - accuracy: 0.9206 - val_loss: 0.2797 - val_accuracy: 0.9216\n","Epoch 153/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2832 - accuracy: 0.9207 - val_loss: 0.2796 - val_accuracy: 0.9216\n","Epoch 154/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2831 - accuracy: 0.9205 - val_loss: 0.2795 - val_accuracy: 0.9216\n","Epoch 155/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2829 - accuracy: 0.9209 - val_loss: 0.2794 - val_accuracy: 0.9216\n","Epoch 156/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2827 - accuracy: 0.9210 - val_loss: 0.2793 - val_accuracy: 0.9216\n","Epoch 157/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2825 - accuracy: 0.9211 - val_loss: 0.2792 - val_accuracy: 0.9215\n","Epoch 158/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2824 - accuracy: 0.9208 - val_loss: 0.2790 - val_accuracy: 0.9218\n","Epoch 159/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2822 - accuracy: 0.9211 - val_loss: 0.2790 - val_accuracy: 0.9212\n","Epoch 160/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2820 - accuracy: 0.9210 - val_loss: 0.2788 - val_accuracy: 0.9217\n","Epoch 161/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2819 - accuracy: 0.9211 - val_loss: 0.2788 - val_accuracy: 0.9217\n","Epoch 162/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2817 - accuracy: 0.9215 - val_loss: 0.2785 - val_accuracy: 0.9215\n","Epoch 163/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2816 - accuracy: 0.9210 - val_loss: 0.2785 - val_accuracy: 0.9215\n","Epoch 164/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2814 - accuracy: 0.9212 - val_loss: 0.2785 - val_accuracy: 0.9218\n","Epoch 165/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2813 - accuracy: 0.9213 - val_loss: 0.2784 - val_accuracy: 0.9217\n","Epoch 166/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2811 - accuracy: 0.9211 - val_loss: 0.2783 - val_accuracy: 0.9216\n","Epoch 167/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2809 - accuracy: 0.9214 - val_loss: 0.2782 - val_accuracy: 0.9218\n","Epoch 168/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2808 - accuracy: 0.9216 - val_loss: 0.2781 - val_accuracy: 0.9216\n","Epoch 169/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2806 - accuracy: 0.9215 - val_loss: 0.2780 - val_accuracy: 0.9217\n","Epoch 170/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2804 - accuracy: 0.9215 - val_loss: 0.2779 - val_accuracy: 0.9214\n","Epoch 171/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2803 - accuracy: 0.9218 - val_loss: 0.2779 - val_accuracy: 0.9214\n","Epoch 172/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2802 - accuracy: 0.9212 - val_loss: 0.2777 - val_accuracy: 0.9218\n","Epoch 173/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2800 - accuracy: 0.9217 - val_loss: 0.2776 - val_accuracy: 0.9214\n","Epoch 174/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2798 - accuracy: 0.9219 - val_loss: 0.2776 - val_accuracy: 0.9219\n","Epoch 175/200\n","375/375 [==============================] - 2s 4ms/step - loss: 0.2797 - accuracy: 0.9218 - val_loss: 0.2775 - val_accuracy: 0.9217\n","Epoch 176/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2795 - accuracy: 0.9217 - val_loss: 0.2774 - val_accuracy: 0.9219\n","Epoch 177/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2794 - accuracy: 0.9219 - val_loss: 0.2773 - val_accuracy: 0.9220\n","Epoch 178/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2793 - accuracy: 0.9218 - val_loss: 0.2772 - val_accuracy: 0.9218\n","Epoch 179/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2791 - accuracy: 0.9218 - val_loss: 0.2770 - val_accuracy: 0.9218\n","Epoch 180/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.2790 - accuracy: 0.9219 - val_loss: 0.2770 - val_accuracy: 0.9218\n","Epoch 181/200\n","375/375 [==============================] - 2s 6ms/step - loss: 0.2789 - accuracy: 0.9220 - val_loss: 0.2769 - val_accuracy: 0.9218\n","Epoch 182/200\n","375/375 [==============================] - 2s 5ms/step - loss: 0.2787 - accuracy: 0.9219 - val_loss: 0.2768 - val_accuracy: 0.9218\n","Epoch 183/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2786 - accuracy: 0.9221 - val_loss: 0.2767 - val_accuracy: 0.9219\n","Epoch 184/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2784 - accuracy: 0.9219 - val_loss: 0.2767 - val_accuracy: 0.9218\n","Epoch 185/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2783 - accuracy: 0.9221 - val_loss: 0.2766 - val_accuracy: 0.9221\n","Epoch 186/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2782 - accuracy: 0.9222 - val_loss: 0.2765 - val_accuracy: 0.9220\n","Epoch 187/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2780 - accuracy: 0.9223 - val_loss: 0.2764 - val_accuracy: 0.9220\n","Epoch 188/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2779 - accuracy: 0.9220 - val_loss: 0.2763 - val_accuracy: 0.9219\n","Epoch 189/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2778 - accuracy: 0.9225 - val_loss: 0.2763 - val_accuracy: 0.9222\n","Epoch 190/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2777 - accuracy: 0.9222 - val_loss: 0.2762 - val_accuracy: 0.9225\n","Epoch 191/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2775 - accuracy: 0.9222 - val_loss: 0.2761 - val_accuracy: 0.9225\n","Epoch 192/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2774 - accuracy: 0.9222 - val_loss: 0.2760 - val_accuracy: 0.9225\n","Epoch 193/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2773 - accuracy: 0.9224 - val_loss: 0.2760 - val_accuracy: 0.9230\n","Epoch 194/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2771 - accuracy: 0.9226 - val_loss: 0.2759 - val_accuracy: 0.9225\n","Epoch 195/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2770 - accuracy: 0.9225 - val_loss: 0.2758 - val_accuracy: 0.9222\n","Epoch 196/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2769 - accuracy: 0.9224 - val_loss: 0.2757 - val_accuracy: 0.9228\n","Epoch 197/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2767 - accuracy: 0.9227 - val_loss: 0.2757 - val_accuracy: 0.9224\n","Epoch 198/200\n","375/375 [==============================] - 1s 3ms/step - loss: 0.2766 - accuracy: 0.9225 - val_loss: 0.2756 - val_accuracy: 0.9223\n","Epoch 199/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.9226 - val_loss: 0.2756 - val_accuracy: 0.9226\n","Epoch 200/200\n","375/375 [==============================] - 1s 4ms/step - loss: 0.2764 - accuracy: 0.9224 - val_loss: 0.2754 - val_accuracy: 0.9227\n","313/313 [==============================] - 1s 2ms/step - loss: 0.2775 - accuracy: 0.9230\n","Test accuracy: 0.9229999780654907\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras.utils import plot_model\n","\n","#신경망과 훈련 매개변수\n","EPOCHS =200 #훈련 얼마나 지속할지\n","BATCH_SIZE = 128 #신경망에 입력하는 표본의 수\n","VERBOSE = 1 \n","NB_CLASSES =10 # 출력 개수(숫자 개수)\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.2 # 검증 위해 훈련 데이터 남김\n","\n","# MNIST 데이터 로드\n","# 검증\n","# 훈련과 테스트 데이터를 각각 60,000개와 10,000개로 나눔\n","# 레이블에 원핫 인코딩은 자동으로 적용\n","mnist = keras.datasets.mnist\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","\n","# X_train 은 60,000개 행으로 28x28 값을 가짐. 이를 60,000*784 형태로 변환\n","#입력계층에 이미지의 각 픽셀과 뉴런 연결\n","RESHAPED = 784\n","X_train = X_train.reshape(60000, RESHAPED)\n","X_test = X_test.reshape(10000, RESHAPED)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","#입력을 [0, 1]로 사이로 정규화\n","X_train /=255\n","X_test /= 255\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')\n","\n","#레이블을 원핫 인코딩\n","Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n","Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n","\n","# 모델 구축\n","# tf.keras 모델 : sequential API(가장 기본적), functional API, Model Subclassing\n","model = tf.keras.models.Sequential()\n","model.add(keras.layers.Dense(NB_CLASSES,\n","                             input_shape = (RESHAPED,),\n","                             name = 'dense_layer',\n","                             activation = 'softmax'))\n","\n","# summary of the model\n","model.summary()\n","\n","# 모델 컴파일(최적화기, 손실 함수, 척도)\n","model.compile(optimizer = 'SGD',  # 최적화기: 확률적 그래디언트 하강(Stochastic Gradient Descent)\n","              loss = 'categorical_crossentropy',\n","              metrics = ['accuracy'])\n","\n","# 모델 훈련\n","model.fit(X_train, Y_train,\n","          batch_size = BATCH_SIZE, epochs = EPOCHS,\n","          verbose = VERBOSE, validation_split = VALIDATION_SPLIT)\n","\n","# 모델 평가\n","test_loss, test_acc = model.evaluate(X_test, Y_test) #손실값(결과값과의 차이)과 정확도\n","print('Test accuracy:', test_acc)\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","model_json = model.to_json()\n","with open('/content/drive/MyDrive/model/Mnist_V1_architecture.json', 'w') as json_file:\n","  json_file.write(model_json)\n","model.save_weights('/content/drive/MyDrive/model/Mnist_V1_weights.h5')"],"metadata":{"id":"S9zdLIoAQ2-D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668666883550,"user_tz":-540,"elapsed":4202,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"870e24dd-0572-4ffb-ed67-ea754a48f820"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["clear"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jR39TwJ8Jn68","executionInfo":{"status":"ok","timestamp":1668669842460,"user_tz":-540,"elapsed":20,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"b0cc2283-f513-4552-e793-34e972b058d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[H\u001b[2J"]}]},{"cell_type":"markdown","source":["입력값 확인"],"metadata":{"id":"DXEH-i5tJqLj"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow import keras\n","\n","\n","#신경망과 훈련 매개변수\n","EPOCHS =200 #훈련 얼마나 지속할지\n","BATCH_SIZE = 128 #신경망에 입력하는 표본의 수\n","VERBOSE = 1 \n","NB_CLASSES =10 # 출력 개수(숫자 개수)\n","N_HIDDEN = 128\n","VALIDATION_SPLIT = 0.2 # 검증 위해 훈련 데이터 남김\n","\n","# MNIST 데이터 로드\n","# 검증\n","# 훈련과 테스트 데이터를 각각 60,000개와 10,000개로 나눔\n","# 레이블에 원핫 인코딩은 자동으로 적용\n","mnist = keras.datasets.mnist\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","\n","for i in range(10):\n","  cv2_imshow(X_test[i])\n","\n","# X_train 은 60,000개 행으로 28x28 값을 가짐. 이를 60,000*784 형태로 변환\n","#입력계층에 이미지의 각 픽셀과 뉴런 연결\n","RESHAPED = 784\n","X_train = X_train.reshape(60000, RESHAPED)\n","X_test = X_test.reshape(10000, RESHAPED)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","#입력을 [0, 1]로 사이로 정규화\n","X_train /=255\n","X_test /= 255\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')\n","\n","#레이블을 원핫 인코딩\n","Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n","Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"id":"mRNnr-g9J0hL","executionInfo":{"status":"ok","timestamp":1668669899523,"user_tz":-540,"elapsed":690,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"e6963fa6-3b44-4ebd-e719-4930c370a422"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17650>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17590>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17710>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A176D0>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA70lEQVR4nMXQsWoCQRQF0JvBNNrGdjcgIR8gJJVrqRZCfkMkgn+QlEIIAcHaHzClVWySJkmXSgxqIbKCbcrLxRSbNe7M2uqr7syZefAecOTK9fTp78MLUs2ds9nJ+b71OPMfWzdXAALz9ZrSVCQpclp0bbiRpPVckmPlmUh268Ed2bDsfEVx2skCfsif9qkzxcsZAOCWYsHGDy+K/nuM2zmNuV5E6cQYc5/4+UDG0W07iTFfXlGhl45PJGelKGeQrOElgPFb8vJbqtWW0kYpG2qT8W7ZtdEP/zAcFbI2IniMsOkIAKD6zEGl6qXjweoXXfV/5XmKZEMAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17710>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2klEQVR4nGNgGLzA6AGU4SYLZTAhJN3ZoQy/bgxJFi8Y64wWN7qko+V8KEtIiwvNRt03N3mgzAN/RNEkV3w3hWn8/xdNMuTTZRiz9+9eVlTJlX+yoCyFF7+cUOX4H/6BMdv+wM2AupZdegVMRJnhCppzOM9cFIKwxP7+zYaJskCo73eDt/YxMDDoKMv/Z/iPppNBc9XXP3/+/Hnx/PefP5wwQUa4tKEyAwPDGoaF0TDTsID6P3900exEAEZGhss4Jf8jOYcJXZKD4QdOKxlevMnHLbnZCbcclQAA/k48Hcv/z+EAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17690>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGOzA9182My454cf//nHikgz8928pIw459tP//nni0mj6798vBI8JzVQGhl24NDIc+ffDAJec1b9/b5G4qMaaMjBMx2nq4n/vZHDJ2fz5dx+Zj2KsMBPDbnymmuKSk/nz7xKKALKxVkwMG3GamvnvlQhOnW4Mjz7ikmRVYfjxG5fkv9MMd1DtYUEw/9b8P4fTPdQEAJbDL46GK5NFAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A173D0>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgGLRA7ECbAozN78uKIif4+tdKuNydj6rIciJ7/06Gc7r/pqBodPv7VxTG1v63lhfFwpl/E+Byz//FoGhc/P8MN4yd8W8eqlMX/d0EdR9n89u/MFEWGMN714fpDAwM9g4WDGtQNTIYP/n799/fv3///vv797Yyms6zugYepa8XMjAsvshw7C4DDqD075woLjmGBX9dccqF/vtohFNy3r+lOOUYnn/BrTHj3wvcGi/8ncvAKwfnMqFJ/43e34xT57+/s2RxSNruaxBnw20rlQAAKNJLfTqR0FsAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17510>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nGNgGKqAb+pBVlxy0Q/+/hXGISfz+t/fv8uEsEtO+Pvv79+/74rZsMjJf/x7Yeffv3+fS2CR9P93kIEj6fa//ycRJjPBGOz/+xl+zLv9//+3X5iSkQzeDAwMJgwMJ75gGhv294JG6LLfb/+90cKUFHr399/fvztVbvydgcVFLh/+/5vIwdD2774yNtl5fTwMDJzr/y7EIgkFEX8f4QgmBgYGpmV/63BrNfj6Vw23bPG/NZw4JUVv/dPDrVXu31Lckgy7vmAJJhjgu++HRysRAAA+/lIBnbxrFgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17610>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoD9gxBARlGNgeFh45dZFTMXeM2/+/fv3+re/f9FllPu+/PkLAwwMDCzIkjL5EPrGVQgNlxQpOLLj18ev3LuunDz//Suqidzn/voxMCgwyDFhuoNt498WLhwe4mn9+5IfhxxDzN/7MliEIVZYMZx/gksjw6u/3+oNMYUhwff/HwPDvxkn5O5cZdA+jm5G918EeLECTZLZ9Na93zDZPzWYFjh7nIDKrsfmrIq/P2cYL8EhafT37989f/7+nYJNknP5379///5az41NkkF8y/O/dxuwSjEwMDDEThXDKUcfAAAG83bQTLLiMgAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7FDCA8A17690>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBElEQVR4nN3QsS8DcRjG8UcOPUFiINLNYGgj2M7QpTGISSIxWRkMFomIGESCwWJ1XTtYDP4Ag60xoqNBqjW0IWlPQ/Tc9wyWa/P7/QOe7c3nfYb3lf5N+rqm6fHVPH7pybA46zcA6JQvBntortCE6uVpeMdrZavbCg24OXel20zp8zmaSJB7GFE/Gpakx5klIInLH9Q8SXKmdisBcXFAkvr/0IkULqxl9JXNvk1K9ZMw0Ry6bkcxhAD8XKV77hzbz72/pOY9Sf5B0/iTIrQ2HCNprwPrZtJmAOWU2bwWBDlL8RjaeYuNfoNvsZEa3LsWXIlh0WJ6gDObqRrX01bcibetZsovhERycinB3ycAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["60000 train samples\n","10000 test samples\n"]}]},{"cell_type":"code","source":["# 테스트 값 형태\n","print(\"X_test type: \", type(X_test))\n","print(\"X_test.shape:\", X_test.shape)\n","print(X_test)\n","\n","print(\"\\nX_test[10] type: \", type(X_test[10]))\n","print(\"X_test[10].shape:\", X_test[10].shape)\n","print(X_test[10]*255)\n","print(Y_test[10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OV5t8rLSz4qQ","executionInfo":{"status":"ok","timestamp":1668672149550,"user_tz":-540,"elapsed":314,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"df1b8911-5b19-4db8-a6a1-e733b30398b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_test type:  <class 'numpy.ndarray'>\n","X_test.shape: (10000, 784)\n","[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]]\n","\n","X_test[10] type:  <class 'numpy.ndarray'>\n","X_test[10].shape: (784,)\n","[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  61.   3.  42.\n"," 118. 193. 118. 118.  61.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.  14. 179. 245. 236. 242.\n"," 254. 254. 254. 254. 245. 235.  84.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0. 151. 254. 254. 254. 213.\n"," 192. 178. 178. 180. 254. 254. 241.  46.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.  43. 235. 254. 226.  64.  28.\n","  12.   0.   0.   2. 128. 252. 255. 173.  17.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.  56. 254. 253. 107.   0.   0.\n","   0.   0.   0.   0.   0. 134. 250. 254.  75.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.  63. 254. 158.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0. 221. 254. 157.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0. 194. 254. 103.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0. 150. 254. 213.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.  34. 220. 239.  58.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  84. 254. 213.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0. 126. 254. 171.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  84. 254. 213.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0. 214. 239.  60.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  84. 254. 213.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0. 214. 199.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  84. 254. 213.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  11. 219. 199.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  84. 254. 213.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  98. 254. 199.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0. 162. 254. 209.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  98. 254. 199.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.  51. 238. 254.  75.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  98. 254. 199.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.  51. 165. 254. 195.   4.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  66. 241. 199.   0.   0.   0.   0.   0.\n","   0.   0.   0.   3. 167. 254. 227.  55.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0. 214. 213.  20.   0.   0.   0.   0.\n","   0.  46. 152. 202. 254. 254.  63.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0. 214. 254. 204. 180. 180. 180. 180.\n"," 180. 235. 254. 254. 234. 156.  10.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.  81. 205. 254. 254. 254. 254. 254.\n"," 254. 254. 252. 234. 120.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.  26. 210. 254. 254. 254. 254.\n"," 254. 153. 104.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n","[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"]}]},{"cell_type":"markdown","source":["img값은 numpy의 ndarray type입니다. numpy는 python에서 수학적 처리를 위한 모듈로 openCV에서도 많이 사용됩니다. img가 어떤 형태의 행렬인지 확인을 해보려면 아래와 같이 입력합니다.\n","\n","\\>>> img.shape\n","(206, 207, 3)"],"metadata":{"id":"ud7SRxTfBW0l"}},{"cell_type":"code","source":["clear"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HhZFayT6ICQL","executionInfo":{"status":"ok","timestamp":1668670098957,"user_tz":-540,"elapsed":875,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"88305067-7fac-4920-abec-df6f35427056"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[H\u001b[2J"]}]},{"cell_type":"markdown","source":["라이브러리 import"],"metadata":{"id":"-RtZPkW9H2ej"}},{"cell_type":"code","source":["# 구글 드라이브 연결\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","# 모델\n","from tensorflow.keras.models import model_from_json\n","from tensorflow.keras.optimizers import SGD\n","\n","# 이미지처리\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from skimage.transform import resize\n","#from imageio import imread"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IPp1VV70pKd","executionInfo":{"status":"ok","timestamp":1669811275047,"user_tz":-540,"elapsed":21611,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"ebf18f63-a8a3-4536-8298-9452a831ab77"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["모델 로드"],"metadata":{"id":"Kj-3GepbH9Wl"}},{"cell_type":"code","source":["# 모델 로드\n","model_architecture = '/content/drive/MyDrive/model/Mnist_V1_architecture.json'\n","model_weights = '/content/drive/MyDrive/model/Mnist_V1_weights.h5'\n","model = model_from_json(open(model_architecture).read())\n","model.load_weights(model_weights)\n","\n","model.summary()\n","\n","# 훈련\n","optim = SGD()\n","model.compile(loss = 'categorical_crossentropy', optimizer = optim,\n","              metrics = ['accuracy'])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcVBtA-RISNP","executionInfo":{"status":"ok","timestamp":1668670126784,"user_tz":-540,"elapsed":13,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"4a7d71c4-7e1e-43d3-a1fc-b1de7b889f09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_layer (Dense)         (None, 10)                7850      \n","                                                                 \n","=================================================================\n","Total params: 7,850\n","Trainable params: 7,850\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["validation data로 model을 제대로 불러왔는지 확인"],"metadata":{"id":"tiOV7gWrQCd6"}},{"cell_type":"code","source":["mnist = keras.datasets.mnist\n","(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n","\n","# X_train 은 60,000개 행으로 28x28 값을 가짐. 이를 60,000*784 형태로 변환\n","#입력계층에 이미지의 각 픽셀과 뉴런 연결\n","RESHAPED = 784\n","X_train = X_train.reshape(60000, RESHAPED)\n","X_test = X_test.reshape(10000, RESHAPED)\n","X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","\n","#입력을 [0, 1]로 사이로 정규화\n","X_train /=255\n","X_test /= 255\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')\n","\n","#레이블을 원핫 인코딩\n","Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\n","Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n","\n","test_loss, test_acc = model.evaluate(X_test, Y_test) #손실값(결과값과의 차이)과 정확도\n","print('Test accuracy:', test_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4oGcmUa0QBTq","executionInfo":{"status":"ok","timestamp":1668670132428,"user_tz":-540,"elapsed":1840,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"bd24fe47-63fd-426f-d2f6-984415f1e396"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["60000 train samples\n","10000 test samples\n","313/313 [==============================] - 1s 2ms/step - loss: 0.2775 - accuracy: 0.9230\n","Test accuracy: 0.9229999780654907\n"]}]},{"cell_type":"markdown","source":["predict 함수 기능 확인\n","predict한 클래스의 값과 원 핫 인코딩 된 Y_test 값 비교했을 때 동일함"],"metadata":{"id":"ulyiTXzWUZzT"}},{"cell_type":"code","source":["prediction = model.predict(X_test[0:10]) \n","classes = np.argmax(prediction, axis=1)\n","\n","#print(\"predictions:\", np.argmax(prediction))\n","print(\"classes:\", classes)\n","\n","print(Y_test[0:10])\n","\n","print(X_test[0:10].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yxghjwu6UZho","executionInfo":{"status":"ok","timestamp":1668670149954,"user_tz":-540,"elapsed":318,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"c63351a6-4f28-4f5b-ecbf-e95a69c41f47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 42ms/step\n","classes: [7 2 1 0 4 1 4 9 6 9]\n","[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n","(10, 784)\n"]}]},{"cell_type":"markdown","source":["이미지 로드"],"metadata":{"id":"tVOuY-5oHz2u"}},{"cell_type":"code","source":["# 이미지 로드\n","#img_names = ['/content/drive/MyDrive/dataset/Mnist/number1_1.png',\n","#             '/content/drive/MyDrive/dataset/Mnist/number5_1.png',\n","#             '/content/drive/MyDrive/dataset/Mnist/number7_1.png',\n","#             '/content/drive/MyDrive/dataset/Mnist/number8_1.png']\n","img_name1 = '/content/drive/MyDrive/dataset/Mnist/number1_1.jpg'\n","img1 = cv2.imread(img_name1, 0) # cv2.imread(img, flag) flag 0: 흑백으로 읽음\n","cv2_imshow(img1)\n","print(\"img type:\", type(img1))\n","print(\"img.shape:\", img1.shape)\n","print(img1)"],"metadata":{"id":"GZa6-uBXHunk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이미지 변환"],"metadata":{"id":"KYeA5nMcI6Go"}},{"cell_type":"code","source":["img1 = img1.astype(\"float32\") # type 변환. 하는 이유는 알아봐야함.\n","#img = resize(imread(img_name), (28, 28, 1)).astype(\"float32\")\n","img1 = img1.reshape(784) # 배열로 변환\n","img1_data = np.array(img1)/255 # 정규화\n","print(\"img1 type: \", type(img1_data))\n","print(\"img1 shape: \", img1_data.shape)\n","print(img1_data)"],"metadata":{"id":"w4AY8LdC_Eee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["다수의 이미지 불러오기"],"metadata":{"id":"SxzXWnFyUybZ"}},{"cell_type":"code","source":["# 이미지 로드\n","img_names = ['/content/drive/MyDrive/dataset/Mnist/number0_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number1_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number2_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number3_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number4_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number5_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number6_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number7_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number8_1.jpg',\n","             '/content/drive/MyDrive/dataset/Mnist/number9_1.jpg']\n","\n","# img = cv2.imread(img_name, 0) # cv2.imread(img, flag) flag 0: 흑백으로 읽음\n","imgs = np.array([cv2.imread(img_name, 0) for img_name in img_names])\n","\n","for i in range(10):\n","  cv2_imshow(imgs[i])\n","\n","print(\"imgs type:\", type(imgs))\n","print(\"imgs.shape:\", imgs.shape)\n","print(imgs)\n","imgs = imgs.reshape((10, 28, 28, 1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"p_sHA-CUU095","executionInfo":{"status":"ok","timestamp":1669811386431,"user_tz":-540,"elapsed":2785,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"05172809-0a7f-4aaf-88e1-57045950cfaa"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F96C54B72D0>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA4klEQVR4nG2SS3LEIAxEX0uQ3P+2qTGos8DOxHi0opDUP4CzEkJEoAQhbpWJvtZQsFVcABHwzd6XRIekP1BRE0q0+PcuAiLX6Q4bQBhQfNgMYtg2BOR/kUDlEJhC7GITbGC4ir7Lwf5JaNi+cN8upxs0MoYfizGthWfT76TChRBa3De1BqUwVj1CwD60zPvJWV7++qemXyFo0D1jT28aukhpes9WL+sUaMcevI9KAiLG+BDfQaK21O6krtKfp42TsMu2a+aZwXtCBl932q3G9U75/JgNSDJJoN0516fs0AEtkF9iFFakD9gYVAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645666090>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAh0lEQVR4nL2SQRLCMAwDV04K//8uUIsL0+mUiCM65LK2JY8DHwmhUrGSTu9Ss7Smqu+2w8NAXQoOKMs4GYrb0z8C+dJ53qteHms4QdIeZs6Jmy3RL89DWzHdKWoVvedV3GZ9FQpHBsMtZsiKH1vy1LDJc/d4FO640zcp3B6MNcVmJKZ2Qv/UG3ARKVDlkfk5AAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645666090>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6klEQVR4nL2RQY7EIBADy27Q/P+3KyXgPUCSUfa+fUCAcanbwC5BFQitle+yBB0V2LzLtAZL+uC3iLAo+puKaeB1XbxVkZn0WvsXNjkzZpD/OpMRkREw1AO0mDMAbQTwZTQFOMd6n7N7D81qrYCOBUqgOm1DmzalAU6WoGd2bQkyQdTdbEcCSgV9nvLXkEWjllM+ska5ZV+nIjNI+DYvvnd8COlOz0twI3OEMkWrJz6rAT+Zu2XX/We1zMmR2iiuEEzDJDPJyJlzZvqzQ5cH5JDNKCIIGOX6ZWaSXMwC14WVdN0vrb5T+O/6Be6AUwACEaRTAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645666090>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2UlEQVR4nMWSwW7EIBBDnz2J9v//tt0muAdgg6rk3BEHJOMZjw3M8jxXbfNSQHDjvrbx5B6z9ICNwfdwb/hELfzAA3Lmr1SDUEEIOgQq0Oi/GYNyBs5Agb2SZRLhvNWh13SqhgxDgih2raolYIecfZmaO2nHgAvSWvq++gRQUMjkOzmDvDA1zCnKJAGvAcgIhAxp6GKigr2jQGK8JldAi8FFi8BsY+YLSrR3F5hA4c8H8UZ1JXXk6A6OmTsSSElraV9JD2LuKTDVkvzEQJUXD6yZHUZcif1H/QLGvkXAzaxpdQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645666090>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA8ElEQVR4nH2SyW7EMAxDH6kk6P9/bdHEZg/O1mlmdLABSdRCEXYTQpZ5Mt3eR5us56j8H3b2COCXhDOoKORdQ7Gs+TBQXpAGQ2FB2uU9N5uZQGQNoFKyHqPUTGGgb6NMsuFrUKCcHglRvXNQ5Z2XlsgUkFbTURVASoJZCtID1hGtGXoEhqWlM/lsaCiSUGjadz3q7iOu2XpakiQ9J3JQUCRpae072dJqAMcJizLoJ/mCWRdDNT5hJ4C4q2WmRlKyGs3nJc0CNqhoyXDfgB6l7dZGYDoEIQSTYCYJOt1/D5tsfqMEK+tHkRTPggeyc3/ZL2AAZJ+aGz5uAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645185C90>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nL2SsYLDMAxCH8jp/3/tDa0jbkhzbePOxwoyIBmeMFUIYWTEJwpsAzpkV7oAn8rPQZvb02GBDSCzgS78gIHJDKshCJQ9GlgrL7Gnk7TOen89FbohoM41EMVMBGSmF0vSICitljXTFAOWQMVPkkwsfBR9V4Q9KnS7rz0HUEq2c1vvYSUMnS503TyWAKVhW2oCZpBoLLQVA+Sh5Wom3dh6BNC4xk0ykz16DZzYCDStkeXZOrX17dg6Pqbw11/0X/gFdTxGhrVAF0MAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9661957650>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2UlEQVR4nG2S0Y7EIAwDxwl0//93b0t8D+2dCtQPCGkU7CTAQwEBDRIxgUBS0iFncin/zly5aIgODiFihiHohH2iYHs5JTysg2NhkQJ8FhC3/cTtr0lE/nvG3Z9cw1sDjQTC5TUldCTIqtqsAoQAW9fsnmpXVx6+hrB4kkCVB30d+J3GttescE2uDP5uWEKUTSD/bMWh8N2GawINbKwcErBUdghsEiWbaSTh03TQ/Oxl9cE20tgquyA5DW1lErQgXHaNevt4gE+X4x3mvZx3HWhdpx4XI02RfgE9FV6fUppHxQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F96C54B72D0>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nG2SQYLDMAgDR4J0///extYekjTuZrnhsSwMIADMFUbId2aKOwTgAxcGNn2YoTfOfEPNGr5tCh1P38jlw1VA75icMJkWNQPIE0+ReUEDNc/aFNCFgETGF23IOwiDILkvGmTNWKBCydvw88GGDPCGSIJf1zcNhjGNeEF2v04vgMZ4z2GQxLQWT/cYezWYTNDdeVGQFDROQmOWSRQYW8yEr14KIaGj0o/bopQonPeD+Zx99szqhxILjSTmr7SOowSQvqmogs5IoYcQqGIELOpZrkh24CkVoHWIa6VQ15o+2AQz+Z/2n9Vc4xe7xFLsZGjJnQAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645666250>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA3UlEQVR4nHWSQZKDQAwDW/KQ/783NVh7CJDAgm9TXbZlaWAvYfFQxQJenjA2+JboBU8M9KIeh0I6fd7zXbgCykPfGqlP0GjT2C7QPms7uRDYmWEwA4X3bS4W4zVBkGgBfAgfIGVFJHMFhtickgCTIJLIGHR0GlEdQ2diwzj0FtTC2jNJZpD49XiAlWRmJomOS74RKq0oiiQuRgmnoUhuw0kz+Oi+0nq9A9A5bdtVpRm6hYh0dBrrL5OUzzvuCwygnsld5AJyNuGntpxs6nqpBlCUDf++diFgYYtsAPwBTIBilFTtvkcAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<PIL.Image.Image image mode=L size=28x28 at 0x7F9645185890>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nMWSy3aEMAxDr+TQ///dKSTqgkCBma6rpXP0sByYEJSQDMY80QADRaPe3qSiQGJ5Y2oyhXgIizVJFwirnUYAJJCMIVKd3EUrL4ske6a77EiQyUDWnBlUYGk0MiDOkQ4oUV/0BIO3FMVy7mlE9Q4FLQGWYxXt62WKZaXM6UoDKT1Zk5l2Em1vi6Mx+G4M0KBUulgiFwtUIgw1m5c8gxfKFoyL0/PsqOh5DjlSi+R5yAMLI/0PZkHP/QtcncMjyO9pkrHX9eEx9N6cxifo0vd/4gfIJ1GetCbXFAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["imgs type: <class 'numpy.ndarray'>\n","imgs.shape: (10, 28, 28)\n","[[[0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  ...\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]]\n","\n"," [[0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  ...\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]]\n","\n"," [[0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  ...\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]]\n","\n"," ...\n","\n"," [[1 0 0 ... 0 0 0]\n","  [0 2 0 ... 0 0 0]\n","  [3 0 2 ... 0 0 0]\n","  ...\n","  [2 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [4 0 1 ... 0 0 0]]\n","\n"," [[0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  ...\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]]\n","\n"," [[0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  ...\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]\n","  [0 0 0 ... 0 0 0]]]\n"]}]},{"cell_type":"markdown","source":["이미지 변환"],"metadata":{"id":"Ml_GHFcRbW2P"}},{"cell_type":"code","source":["imgs = imgs.astype(\"float32\") # type 변환. 하는 이유는 알아봐야함.\n","#img = resize(imread(img_name), (28, 28, 1)).astype(\"float32\")\n","imgs = imgs.reshape(10, 784) # 배열로 변환\n","print(imgs[7])\n","imgs_data = np.array(imgs)/255 # 정규화\n","print(\"img type: \", type(imgs_data))\n","print(\"img shape: \", imgs_data.shape)\n","print(imgs_data)\n","\n","print(\"img[0] type: \", type(imgs_data[0]))\n","print(\"img[0] shape: \", imgs_data[0].shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gncOJnCUbYzo","executionInfo":{"status":"ok","timestamp":1668672333907,"user_tz":-540,"elapsed":5,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"491a54df-c741-4502-b32d-4f7dcaa73815"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[  1.   0.   0.   0.   0.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   2.   0.   1.   0.   1.   2.   0.   0.   0.   0.   0.   0.\n","   0.   2.   0.   2.   0.   3.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   1.   0.   0.   0.   0.   2.   2.   0.   0.   0.   0.\n","   3.   0.   2.   0.   0.   0.   5.   1.   0.   0.   0.   0.   0.   0.\n","   0.   0.   1.   2.   0.   0.   4.   5.   0.   1.   0.   0.   0.   0.\n","   0.   5.   0.   1.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   2.   0.   0.   0.   2.   0.   0.   0.   0.   0.\n","   3.   0.   1.   0.   0.   2.   0.   2.   0.   0.   0.   0.   0.   0.\n","   0.   0.   2.   0.   0.   2.   3.   2.   1.   2.   0.   0.   0.   0.\n","   0.   0.   0.   4.   0.   0.   2.   2.   1.   1.   1.   1.   1.   1.\n","   1.   1.   0. 255. 253. 255.   0.   0.   3.   1.   0.   0.   0.   0.\n","   2.   0.   0.   2.   0.   1.   1.   0. 254. 254. 254. 254. 254. 254.\n"," 254. 254. 253. 255. 255. 255. 255.   2.   0.   0.   0.   0.   0.   0.\n","   0.   1.   0.   0.   0.   1. 255. 255. 255. 255. 255. 255. 255. 255.\n"," 255. 255. 255. 254. 253. 254.   0.   0.   2.   0.   0.   0.   0.   0.\n","   1.   0.   4.   0.   0. 255. 251. 255.   0.   1.   0.   2.   0.   0.\n","   1.   0.   0. 255. 255.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   2.   0.   0.   1.   2.   1. 254. 255.   2.   1.   0.   0.   1.   3.\n","   0.   1. 255. 255. 251.   2.   0.   0.   7.   0.   0.   0.   0.   0.\n","   0.   0.   2.   2.   0.   0. 255. 253.   0.   0.   2.   5.   0.   1.\n","   0. 255. 255. 255.   0.   2.   6.   0.   1.   0.   0.   0.   0.   0.\n","   2.   0.   0.   2.   0.   0. 253. 254.   2.   0.   1.   0.   6.   0.\n","   0. 255. 252.   2.   6.   0.   0.   1.   0.   4.   0.   0.   0.   0.\n","   0.   4.   0.   2.   0.   2. 252. 255.   0.   0.   2.   0.   0.   1.\n"," 255. 255. 255.   2.   0.   4.   1.   0.   0.   0.   0.   0.   0.   0.\n","   2.   0.   0.   2.   4. 253. 253. 252.   3.   4.   0.   0.   2.   0.\n"," 255. 254.   0.   0.   1.   1.   0.   1.   2.   0.   0.   0.   0.   0.\n","   1.   0.   3.   0.   0. 255. 255.   3.   0.   0.   4.   0.   2. 255.\n"," 255. 255.   0.   4.   0.   2.   0.   0.   3.   0.   0.   0.   0.   0.\n","   0.   0.   0.   3.   0.   0.   2.   0.   2.   2.   1.   0. 254. 255.\n"," 255.   0.   0.   0.   2.   0.   2.   0.   0.   0.   0.   0.   0.   0.\n","   1.   0.   1.   0.   1.   1.   0.   1.   0.   1.   0. 255. 255. 255.\n","   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   3.   0.   1.   1.   0.   3.   0.   2. 255. 251.   1.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   2.   2.   0.   1.   0.   1.   2.   0. 255. 252. 255. 254.   3.\n","   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   2.   1.   0.   1. 253. 255. 255. 255.   2.   0.\n","   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   3.   2.   0.   1.   0.   0.   0. 255. 255.   0.   0.   0.   1.\n","   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   1.   0.   3.   3.   0.   0.   4. 255. 253. 255.   3.   0.   1.   1.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   3.   0.   0.   0.   3.   3.   0. 253. 255.   0.   0.   2.   1.   0.\n","   3.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   1.   0. 255. 255. 252.   0.   0.   0.   0.   1.\n","   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   1.   0.   0.   0.   1. 255. 255.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   2.   0.   0.   3.   0.   2.   2.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   5.   0.   0.   2.   2.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   4.   0.   1.   0.   0.   2.   0.   2.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n","img type:  <class 'numpy.ndarray'>\n","img shape:  (10, 784)\n","[[0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," ...\n"," [0.00392157 0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]\n"," [0.         0.         0.         ... 0.         0.         0.        ]]\n","img[0] type:  <class 'numpy.ndarray'>\n","img[0] shape:  (784,)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"1_VFtvacbYIm"}},{"cell_type":"code","source":["prediction = model.predict(imgs_data) \n","classes = np.argmax(prediction, axis=1)\n","\n","#print(\"predictions:\", np.argmax(prediction))\n","print(\"classes:\", classes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ri0cqGgKv2zP","executionInfo":{"status":"ok","timestamp":1668672351439,"user_tz":-540,"elapsed":323,"user":{"displayName":"고다미","userId":"14264368571070937638"}},"outputId":"be59be11-0242-4c7e-cfba-d29656cf72c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 29ms/step\n","classes: [0 5 2 5 4 3 5 8 8 8]\n"]}]}]}